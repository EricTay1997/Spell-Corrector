{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/erictay1997/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "corrupted_file = open('ausen-sense-corrupted.txt').read()\n",
    "correct_file = nltk.corpus.gutenberg.raw('austen-sense.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(textFile):\n",
    "    words = re.findall(r\"[\\w']+|[\\n-.\\\",!?:;\\[\\]]\", textFile)\n",
    "    tokens = []\n",
    "    for word in words:\n",
    "        foundContraction = re.search(r\"(\\w+)('ll|'LL|'re|'RE|'ve|'VE|n't|N'T|'s|'S|'d|'D|'m|'M|'a|')\", word)\n",
    "        if foundContraction:\n",
    "            tokens.append(foundContraction.group(1))\n",
    "            tokens.append(foundContraction.group(2))\n",
    "        else:\n",
    "            tokens.append(word)\n",
    "    \n",
    "    print(\"finished tokenizing\")\n",
    "    print(len(tokens))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished tokenizing\n",
      "267648\n",
      "finished tokenizing\n",
      "267710\n"
     ]
    }
   ],
   "source": [
    "corrupted_tokens = tokenizer(corrupted_file)\n",
    "correct_tokens = tokenizer(correct_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make same length for alignment\n",
    "for i in range(62):\n",
    "    corrupted_tokens.append('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "# These spaces are to handle misalignment issues\n",
    "# This occurs when the corrupted file deletes words, which we cannot correct\n",
    "counter = 0\n",
    "for i in range(len(correct_tokens)):\n",
    "    if corrupted_tokens[i] != correct_tokens[i]:\n",
    "        if correct_tokens[i] == \" \":\n",
    "            counter += 1\n",
    "            corrupted_tokens.insert(i, \" \")\n",
    "            corrupted_tokens.pop()\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18929"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 18929 tokens that are different/corrupted\n",
    "df = pd.DataFrame({'corrupted' : corrupted_tokens, 'correct' : correct_tokens }, columns=['corrupted','correct'])\n",
    "misaligned = df[df['corrupted'] != df['correct']]\n",
    "len(misaligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print for Sanity Check\n",
    "# for i in range(len(misaligned)):\n",
    "#     print(misaligned.iloc[i,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = open('dictionary.txt').read().split('\\n')\n",
    "# dictionary.txt is a txt file of all valid words taken from the link on Piazza\n",
    "# https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('unigram_freq.csv', index_col = False)\n",
    "# unigram_freq.csv is a csv file from Kaggle, containing english words and word frequency\n",
    "# Data is derived from the Google Web Trillion Word Corpus.\n",
    "# https://www.kaggle.com/rtatman/english-word-frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['word'].isin(all_words)] # Clean kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = {}\n",
    "for i in range(len(df)):\n",
    "    word_counter[df.iloc[i,0]] = df.iloc[i,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in all_words:\n",
    "    if word not in word_counter:\n",
    "        word_counter[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_corrector(word_list):\n",
    "    return [correct(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrects word if it's alphanumeric\n",
    "# Else, do nothing\n",
    "# Do not correct valid words or numbers\n",
    "# Capitalizes it accordingly\n",
    "def correct(word):\n",
    "    if not word.isalnum():\n",
    "        return word\n",
    "    if word.lower() in word_counter:\n",
    "        return word\n",
    "    if word.isdigit():\n",
    "        return word\n",
    "    corrected_word = best_candidate(word.lower())\n",
    "    if word.isupper():\n",
    "        return corrected_word.upper()\n",
    "    if word[0].isupper():\n",
    "        return corrected_word[0].upper() + corrected_word[1:]\n",
    "    return corrected_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns best candidate for a given word\n",
    "# Prioritizes lower Levenshtein distance, and then word frequency\n",
    "# If there are no words with Levenshtein distance â‰¤ 2 in word_counter, return the word itself\n",
    "# We return the word because increasing the max distance would tend to correct proper nouns, etc.\n",
    "def best_candidate(word):\n",
    "    return (best_candidate_from_list(distance1(word)) or best_candidate_from_list(distance2(word)) or word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns best candidate from a list of words\n",
    "# Weighted by word frequency\n",
    "def best_candidate_from_list(words):\n",
    "    count = -1\n",
    "    candidate = None\n",
    "    for w in words:\n",
    "        if w in word_counter and word_counter[w] > count:\n",
    "            count = word_counter[w]\n",
    "            candidate = w\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of words of one Levenshtein distance from word\n",
    "def distance1(word):\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    ret = set()\n",
    "    for i in range(len(word)):\n",
    "        ret.add(word[:i] + word[i+1:]) #Deletion\n",
    "        for letter in letters:\n",
    "            ret.add(word[:i] + letter + word[i:]) #Insertion\n",
    "            ret.add(word[:i] + letter + word[i+1:]) #Substitution\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of words of two Levenshtein distance from word\n",
    "def distance2(word): \n",
    "    ret = set()\n",
    "    for distance1_word in distance1(word):\n",
    "        ret.update(distance1(distance1_word))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the tokens are are different\n",
    "corrupted_tokens_inaccurate = []\n",
    "correct_tokens_shortlist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corrupted_tokens)):\n",
    "    if corrupted_tokens[i] != correct_tokens[i]:\n",
    "        corrupted_tokens_inaccurate.append(corrupted_tokens[i])\n",
    "        correct_tokens_shortlist.append(correct_tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_tokens = spell_corrector(corrupted_tokens_inaccurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in range(len(correct_tokens_shortlist)):\n",
    "    if correct_tokens_shortlist[i] == corrected_tokens[i]:\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6259707327381266"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We correct 63% of tokens that were corrupted\n",
    "counter/len(correct_tokens_shortlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7080"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7080 tokens are still different\n",
    "df = pd.DataFrame({'corrupted' : corrected_tokens, 'correct' : correct_tokens_shortlist }, columns=['corrupted','correct'])\n",
    "misaligned = df[df['corrupted'] != df['correct']]\n",
    "len(misaligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corrupted</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stat</td>\n",
       "      <td>estate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nd</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>te</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>o</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   corrupted correct\n",
       "1          i      in\n",
       "5       stat  estate\n",
       "7         nd     and\n",
       "15        te     the\n",
       "18         o      of"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our spell corrector works, but either the corrupted word is a valid word\n",
    "# Or we correct to another word, not the correct word\n",
    "misaligned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_tokens_all = spell_corrector(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7783"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7783 are different, so our spell corrector doesn't change too many 'correct' words\n",
    "df = pd.DataFrame({'corrupted' : corrected_tokens_all, 'correct' : correct_tokens }, columns=['corrupted','correct'])\n",
    "misaligned = df[df['corrupted'] != df['correct']]\n",
    "len(misaligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267710"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 267710 tokens altogether, of which 7833 are different\n",
    "len(corrected_tokens_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenizer(file_name):\n",
    "    f = open(file_name, \"w\")\n",
    "    for token in corrected_tokens_all:\n",
    "        f.write(token)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenizer(\"Spell_Corrector_corrected_file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sense and Sensibility by Jane Austin 1811]\n",
      "\n",
      "CHAPTER 1\n",
      "\n",
      "\n",
      "The family of Basswood had long been settled i Sussex.\n",
      "Their estate was large, and their residence was at Norland Park,\n",
      "in the centre of their property, where, for many generations,\n",
      "they had lived in so respectable a manner as to engage\n",
      "the general good opinion of their surrounding acquaintance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"Spell_Corrector_corrected_file.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "print(\"\".join(lines[:10]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sense and Sensibility by Jane Austen 1811]\n",
      "\n",
      "CHAPTER 1\n",
      "\n",
      "\n",
      "The family of Dashwood had long been settled in Sussex.\n",
      "Their estate was large, and their residence was at Norland Park,\n",
      "in the centre of their property, where, for many generations,\n",
      "they had lived in so respectable a manner as to engage\n",
      "the general good opinion of their surrounding acquaintance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(correct_file[:355])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
